+++
title = "Lab 4: Earth monitoring with satellite images and Amazon SageMaker"
description = "In this workshop we will build a computer vision processing pipeline for satellite images and use Amazon SageMaker to build a segmentation model."
date = 2019-11-18T08:23:04+11:00
weight = 40
chapter = false
difficulty = "Intermediate / Advanced"
CFTemplate = "Workshop-test.yml"
CFTemplateName = "elasticsearch-cluster"
time = "1hr"
inlists = true
+++

Satellite imagery can be used to solve many problems in earth monitoring including disaster planning and recovery, economic analysis, environmental monitoring and defense. The large volume of images generated by satellites on a daily basis requires automating processes to detect and classify features in high resolution images.

In this workshop we will build a computer vision processing model for satellite images and use Amazon SageMaker to build a segmentation model.


![](/images/computer-vision-sagemaker/SAR.png)
> Sample radar data from satellite


In this workshop, we will demonstrate how to train and host a semantic segmentation model using the Semantic Segmentation built-in algorithm with Deeplab-v3 backbone on a SpaceNet dataset to train a building segmentation model.

We will use the SpaceNet SN6: Multi-Sensor All-Weather Mapping dataset. The data is hosted on AWS as a [public dataset on Amazon S3](https://registry.opendata.aws/spacenet/). 

Synthetic Aperture Radar (SAR) is a unique form of radar that can penetrate clouds, collect during all- weather conditions, and capture data day and night. In this example, we will process it like a regular image and focus on demonstrating the use of the SageMaker to build and host a Semantic Segmentation model.

The data is over 120 sq km of both high resolution synthetic aperture radar (SAR) data and electro optical (EO) imagery with ~48,000 building footprint labels of Rotterdam, The Netherlands

![](/images/computer-vision-sagemaker/overview-1.png)

We will start by assembling a training dataset in the format that the training job will consume. After training a model, we will demonstrate how to host and validate the trained model.

The SN6 dataset is 39.0 GB. For the purpose of this workshop and keeping training time short, we will only use a subset of the data to train the algorithm but feel free to modify the notebook to use to entire dataset.

### Lab Overview

{{< video "SemanticSegmentation_Tash_002.mp4" >}}

>  **Lab overview** 


### Workshop scope

The scope of this workshop is to:

* Understand how to use data from the Open Data Registry in Amazon SageMaker
* Understand how to use Amazon SageMaker built in algorithms, specifically for semantic segmentation
* Going further: Use the full dataset and setup an hyperparameter job to find the best model

### Lab Steps
{{% children depth="2" %}}


Click [here](./step1/) to get started!
